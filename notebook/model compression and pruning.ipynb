{"cells":[{"cell_type":"markdown","metadata":{"id":"-OFFKxGndMRI"},"source":["# Exploring Model Compression Techniques with Bitsandbytes\n","\n","Welcome to this notebook on model compression! In this notebook, we’ll explore state-of-the-art model compression techniques using bitsandbytes and Hugging Face's transformers library. We’ll apply different quantization methods, observe model size reduction, and evaluate each model's effectiveness. Compression techniques like quantization and pruning help reduce memory usage and improve inference speed, especially important for deploying large models on resource-limited devices.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8Vfp5pir2qg","outputId":"a0669ad9-d964-4029-c15c-9df4df6b35e0","executionInfo":{"status":"ok","timestamp":1731408337755,"user_tz":-60,"elapsed":13675,"user":{"displayName":"Andres Occhipinti","userId":"18400320838177687222"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.44.1\n"]}],"source":["!pip install bitsandbytes\n","#!pip install huggingface transformers bitsandbytes>=0.39.0 accelerate datasets torch==2.5.0 --index-url https://download.pytorch.org/whl/cu121\n","# if on Google colab, you need to restart the runtime after the install to reload all the libraries\n","# If on windows\n","# !pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"]},{"cell_type":"markdown","metadata":{"id":"-0G53cPaxOd3"},"source":["## Model Setup\n","\n","Let's start by loading a simple model to work with. We'll use the BERT model from Hugging Face's Transformers library, which is commonly used for natural language processing tasks. We’ll focus on compressing this model using various techniques.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ly_A90N2-bBD"},"outputs":[],"source":["from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForMaskedLM\n","import torch\n","\n","# Replace sequence classification model with masked language model\n","model_name = \"bert-base-cased\"  # or another suitable MLM model\n","model = AutoModelForMaskedLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"5nSHCMjnxRtr"},"source":["## Quantization with bitsandbytes\n","\n","Quantization reduces the model’s weight precision, allowing us to store weights in a lower-bit format (e.g., 8-bit, 4-bit) rather than the standard 32-bit float. This can significantly reduce the model size and improve inference speed without severely impacting model performance.\n","\n","Using `bitsandbytes`, we can easily apply quantization methods like 8-bit and 4-bit quantization. Here, we’ll explore different formats and evaluate the impact on model size and accuracy.\n"]},{"cell_type":"markdown","metadata":{"id":"qKkdKHg62rVO"},"source":["### Default format - 32-bit floating point (FP32)\n","FP32, or 32-bit floating point, is a standard format for representing real numbers in deep learning models. It uses 32 bits split into three components:\n","- **Sign bit**: 1 bit\n","- **Exponent**: 8 bits\n","- **Mantissa (fraction)**: 23 bits\n","\n","This configuration provides a high dynamic range and precision, making it suitable for training large models, though it requires more memory and computational power.\n","\n","**Example**: Representing the number 5.25 in FP32:\n","- **Binary representation**: `0100 0000 1010 1000 0000 0000 0000 0000`\n","- **Explanation**:\n","  - Sign bit = `0` (positive)\n","  - Exponent = `10000001` (biased exponent of 129, or 2^2)\n","  - Mantissa = `01010000000000000000000` (representing 1.3125 in normalized format)\n"]},{"cell_type":"markdown","metadata":{"id":"nfO8EtlFzdlD"},"source":["### Applying FP16 Quantization\n","\n","FP16, or 16-bit floating point, reduces the bit-width to 16 bits:\n","- **Sign bit**: 1 bit\n","- **Exponent**: 5 bits\n","- **Mantissa (fraction)**: 10 bits\n","\n","This format halves the memory requirements compared to FP32, with enough precision for many deep learning tasks. It’s commonly used in mixed-precision training.\n","\n","**Example**: Representing the number 5.25 in FP16:\n","- **Binary representation**: `0100 0101 0100 0000`\n","- **Explanation**:\n","  - Sign bit = `0` (positive)\n","  - Exponent = `10001` (biased exponent of 20, or 2^2)\n","  - Mantissa = `0101000000` (representing 1.313 in normalized format)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gChqci-V8M9u"},"outputs":[],"source":["# Load a model with FP16 precision\n","quantized_model_fp16 = AutoModelForMaskedLM.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"markdown","metadata":{"id":"AiimKPMBzHMy"},"source":["### Applying 8-Bit Quantization\n","INT8 uses 8 bits in an integer format, typically signed:\n","- **Sign bit**: 1 bit\n","- **Value bits**: 7 bits\n","\n","Values range from -128 to 127, or -127 to 127, dependening on scheme. INT8 significantly reduces model size and inference time, though precision loss may impact model accuracy.\n","\n","**Example**: Representing the number 5.25 in INT8 (automatically converted to integer):\n","- **Binary representation**: `0000 0101`\n","- **Explanation**:\n","  - Sign bit = `0` (positive)\n","  - Value = `0000101` (represents 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yM7_AQR_xS48"},"outputs":[],"source":["# Define the quantization configuration for 8-bit\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","# Load an 8-bit quantized version of the model\n","quantized_model_8bit = AutoModelForMaskedLM.from_pretrained(\n","    model_name,\n","    quantization_config=quantization_config\n",")"]},{"cell_type":"markdown","metadata":{"id":"T8uerT-zzNhu"},"source":["### Applying 4-Bit Quantization\n","INT4 compresses data further by using only 4 bits:\n","- **Sign bit**: 1 bit\n","- **Value bits**: 3 bits\n","\n","With values ranging from -8 to 7, INT4 provides high memory savings and fast computations. However, it has limited precision and can lead to quantization errors.\n","\n","**Example**: Representing the number 5.25 in INT4:\n","- **Binary representation**: `0101`\n","- **Explanation**:\n","  - Sign bit = `0` (positive)\n","  - Value = `101` (represents 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9t28KT4zR6y"},"outputs":[],"source":["# Define the quantization configuration for 4-bit\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# Load a 4-bit quantized version of the model\n","quantized_model_4bit = AutoModelForMaskedLM.from_pretrained(\n","    model_name,\n","    quantization_config=quantization_config\n",")"]},{"cell_type":"markdown","metadata":{"id":"V5ZuTEhNzg3P"},"source":["### Applying NF4 Quantization\n","\n","NF4 is a 4-bit floating-point format optimized for normally distributed data:\n","- **Sign bit**: 1 bit\n","- **Exponent**: 2 bits\n","- **Mantissa (fraction)**: 1 bit\n","\n","NF4 offers a compact floating-point representation that maintains a dynamic range, suitable for data typically centered around zero, and is useful for quantization while preserving model accuracy.\n","\n","**Example**: Representing the number 5.25 in NF4:\n","- **Binary representation**: `01 11`\n","- **Explanation**:\n","  - Sign bit = `0` (positive)\n","  - Exponent = `11` (biased exponent giving a larger scale factor)\n","  - Mantissa = `1` (representing a higher precision value near 5)\n","\n","A greater in detail explanation can be found [here](https://www.youtube.com/watch?v=TPcXVJ1VSRI&t=563s)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyWj26yH_Rpy"},"outputs":[],"source":["%%html\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TPcXVJ1VSRI?si=viO6F-ni-_B1SEyH\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWkrF4xizjLM"},"outputs":[],"source":["# Load a 4-bit NF4 quantized version of the model\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',  # Specify NF4 quantization\n",")\n","quantized_model_nf4 = AutoModelForMaskedLM.from_pretrained(\n","    model_name,\n","    num_labels=2,\n","    quantization_config=bnb_config,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"ifrmlnZNzkO_"},"source":["### Comparing model sizes and effectiveness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5Uw2yA8znbS"},"outputs":[],"source":["original_size = model.get_memory_footprint() / (1024 * 1024)\n","quantized_8bit_size = quantized_model_8bit.get_memory_footprint() / (1024 * 1024)\n","quantized_4bit_size = quantized_model_4bit.get_memory_footprint() / (1024 * 1024)\n","quantized_fp16_size = quantized_model_fp16.get_memory_footprint() / (1024 * 1024)\n","quantized_nf4_size = quantized_model_nf4.get_memory_footprint() / (1024 * 1024)\n","\n","print(f\"Original Model Size (32-bit): {original_size:.2f} MB\")\n","print(f\"8-Bit Quantized Model Size: {quantized_8bit_size:.2f} MB\")\n","print(f\"4-Bit Quantized Model Size: {quantized_4bit_size:.2f} MB\")\n","print(f\"FP16 Quantized Model Size: {quantized_fp16_size:.2f} MB\")\n","print(f\"NF4 Quantized Model Size: {quantized_nf4_size:.2f} MB\")\n"]},{"cell_type":"markdown","metadata":{"id":"kiJSEgkW-bBE"},"source":["Theoretically, reducing the bit precision of model parameters should lead to a proportional decrease in model size. For example:\n","\n","- 32-bit to 8-bit: A 4x reduction in size.\n","- 32-bit to 4-bit: An 8x reduction in size.\n","- 32-bit to FP16 (16-bit): A 2x reduction in size.\n","\n","In practice, however, the size reductions achieved are often not exact multiples of the theoretical values, as seen in the models above. This discrepancy arises from additional overhead introduced by quantization schemes, which require extra information to accurately represent the quantized values. This metadata includes scaling factors (such as the alphas in AbsMax) that map high-precision values to lower precision and other essential data needed for quantization. This information is stored alongside the quantized weights, contributing to the overall model size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RscY9qGM-bBF"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Load a dataset suitable for MLM evaluation\n","dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:10%]\")  # Load a subset for quicker evaluation\n","\n","import time\n","\n","def evaluate_model_efficiency(model, dataset, tokenizer):\n","    \"\"\"Evaluates the model on a masked language modeling task and measures inference time.\"\"\"\n","    model.eval()\n","    device = model.device\n","\n","    total_loss = 0.0\n","    num_batches = 0\n","    start_time = time.time()\n","\n","    for i, example in enumerate(dataset):\n","        # Tokenize with special tokens for MLM\n","        inputs = tokenizer(\n","            example[\"text\"],\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=128,\n","        ).to(device)\n","\n","        # Mask the input for MLM evaluation\n","        labels = inputs[\"input_ids\"].clone()\n","        mask = torch.rand(inputs[\"input_ids\"].shape).to(device) < 0.15  # Mask 15% of tokens\n","        labels[~mask] = -100  # Only compute loss for masked tokens\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs, labels=labels)\n","            total_loss += outputs.loss.item()\n","\n","        num_batches += 1\n","\n","    # Calculate average loss and inference time\n","    avg_loss = total_loss / num_batches\n","    perplexity = torch.exp(torch.tensor(avg_loss))\n","    avg_inference_time = (time.time() - start_time) / num_batches\n","\n","    return perplexity.item(), avg_inference_time\n","\n","# Original Model\n","original_perplexity, original_time = evaluate_model_efficiency(model, dataset, tokenizer)\n","print(f\"Original Model - Perplexity: {original_perplexity:.2f}, Avg Inference Time: {original_time:.4f} s\")\n","\n","# Quantized Models\n","# Use only a subset of the dataset for quicker evaluation\n","quantized_16bit_perplexity, quantized_16bit_time = evaluate_model_efficiency(quantized_model_fp16, dataset.select(range(10)), tokenizer)\n","print(f\"fp16 Quantized Model - Perpleity: {quantized_16bit_perplexity:.2f}, Avg Inference Time: {quantized_16bit_time:.4f} s\")\n","\n","quantized_8bit_perplexity, quantized_8bit_time = evaluate_model_efficiency(quantized_model_8bit, dataset, tokenizer)\n","print(f\"8-Bit Quantized Model - Perplexity: {quantized_8bit_perplexity:.2f}, Avg Inference Time: {quantized_8bit_time:.4f} s\")\n","\n","quantized_4bit_perplexity, quantized_4bit_time = evaluate_model_efficiency(quantized_model_4bit, dataset, tokenizer)\n","print(f\"4-Bit Quantized Model - Perplexity: {quantized_4bit_perplexity:.2f}, Avg Inference Time: {quantized_4bit_time:.4f} s\")\n","\n","quantized_nf4_perplexity, quantized_nf4_time = evaluate_model_efficiency(quantized_model_nf4, dataset, tokenizer)\n","print(f\"NF4 Quantized Model - Perplexity: {quantized_nf4_perplexity:.2f}, Avg Inference Time: {quantized_nf4_time:.4f} s\")\n"]},{"cell_type":"markdown","metadata":{"id":"HVQaHZUi-bBF"},"source":["- Why does quantizing a model sometimes impact its accuracy?\n",">>> Write your answer here\n","\n","- What is the primary trade-off when using a lower bit-width, such as 4-bit or 8-bit quantization, instead of the standard 32-bit?\n",">>> Write your answer here\n","\n","- What is the difference between INT8 quantization and FP16 quantization?\n",">>> Write your answer here\n","\n","- Which quantization format would you choose based on your results, and why? Does your result match your expectations?\n",">>> Write your answer here\n"]},{"cell_type":"markdown","metadata":{"id":"CKnGZJvN-bBF"},"source":["## Model Pruning\n","\n","Pruning is a technique to reduce the size of a neural network model by removing parts of the model that have minimal impact on its performance. Common pruning methods include:\n","- **Unstructured Pruning**: Removes individual weights based on a specified criterion, such as the smallest absolute weights. This results in a sparse network.\n","- **Structured Pruning**: Removes entire structures, like neurons or channels, making the model smaller in a way that's compatible with hardware acceleration.\n","\n","Pruning can reduce both the memory and computational requirements of a model, but it may come at the cost of reduced accuracy. In this section, we’ll explore the effects of unstructured pruning on model size and performance. We'll use magnitude pruning, also called L1-norm pruning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YAPasVO-bBF"},"outputs":[],"source":["import torch.nn.utils.prune as prune\n","import copy\n","\n","# Define functions for unstructured and structured pruning\n","def apply_unstructured_pruning(model, amount=0.2):\n","    \"\"\"\n","    Applies unstructured pruning to the model's linear layers.\n","\n","    Args:\n","        model: The model to prune.\n","        amount: The proportion of weights to prune (default is 0.2).\n","\n","    Returns:\n","        The unstructured-pruned model.\n","    \"\"\"\n","    pruned_model = copy.deepcopy(model)\n","    for name, module in pruned_model.named_modules():\n","        if isinstance(module, torch.nn.Linear):\n","            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n","    return pruned_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hCBWjaC-bBF"},"outputs":[],"source":["# Set device to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move the model to the device\n","model = model.to(device)\n","\n","# Apply different levels of unstructured pruning\n","unstructured_pruned_20 = apply_unstructured_pruning(model, amount=0.2)\n","unstructured_pruned_40 = apply_unstructured_pruning(model, amount=0.4)\n"]},{"cell_type":"markdown","metadata":{"id":"SbqRRKXd-bBF"},"source":["### Comparing model sizes and effectiveness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdsTezUP-bBF"},"outputs":[],"source":["# Calculate model sizes\n","original_size = model.get_memory_footprint() / (1024 * 1024)\n","unstructured_20_size = unstructured_pruned_20.get_memory_footprint() / (1024 * 1024)\n","unstructured_40_size = unstructured_pruned_40.get_memory_footprint() / (1024 * 1024)\n","\n","# Print model sizes\n","print(f\"Original Model Size: {original_size:.2f} MB\")\n","print(f\"Unstructured Pruned Model (20%): {unstructured_20_size:.2f} MB\")\n","print(f\"Unstructured Pruned Model (40%): {unstructured_40_size:.2f} MB\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqpheQft-bBF"},"outputs":[],"source":["# Original Model\n","print(f\"Original Model - Perplexity: {original_perplexity:.2f}, Avg Inference Time: {original_time:.4f} s\")\n","\n","# Quantized Models\n","unstructured_20_perplexity, unstructured_20_time = evaluate_model_efficiency(unstructured_pruned_20, dataset, tokenizer)\n","print(f\"Unstructured pruned 20% - Perplexity: {unstructured_20_perplexity:.2f}, Avg Inference Time: {unstructured_20_time:.4f} s\")\n","\n","unstructured_40_perplexity, unstructured_40_time = evaluate_model_efficiency(unstructured_pruned_40, dataset, tokenizer)\n","print(f\"Unstructured pruned 40% - Perplexity: {unstructured_40_perplexity:.2f}, Avg Inference Time: {unstructured_40_time:.4f} s\")"]},{"cell_type":"markdown","metadata":{"id":"pT_XGeGD-bBF"},"source":["- How does pruning impact model size compared to the original model? Do the results meet your expectations?\n",">>> Write your answer here\n","\n","- Which technique—pruning or quantization—resulted in a greater reduction in model size? Why might this be the case?\n",">>> Write your answer here\n","\n","- In terms of perplexity, did pruning or quantization show a greater impact on model accuracy? Why?\n",">>> Write your answer here\n","\n","- If you were looking to optimize both model size and inference time without sacrificing too much accuracy, would you choose pruning or quantization based on your results?\n",">>> Write your answer here"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"NLP2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}