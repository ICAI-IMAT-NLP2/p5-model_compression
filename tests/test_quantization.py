import pytest
import torch
import torch.nn as nn
from src.quantization import QuantizedVector, mixed_precision_forward

@pytest.mark.parametrize("quantization_format", ["int8", "fp16", "1bit", "1.58bit"])
def test_quantized_vector(quantization_format):
    """Tests the quantization and dequantization functionality of the QuantizedVector class."""
    vector = torch.tensor([0.5, -0.75, 1.25, -1.5, 2.0], dtype=torch.float32)
    qv = QuantizedVector(vector, quantization_format=quantization_format)
    
    # Verify quantization
    quantized = qv.get_quantized()
    assert quantized is not None, f"{quantization_format} quantization failed: No quantized tensor returned."
    
    # Dequantize and compare to original vector
    if quantization_format == "int8":
        expected_quantized = torch.tensor([ 32, -48,  79, -95, 127], dtype=torch.int8)
        assert quantized.dtype == torch.int8, f"{quantization_format} quantization failed: Incorrect dtype."
        assert torch.equal(quantized, expected_quantized), f"Fail quantizing vector to {quantization_format} format."
        
        dequantized = qv.dequantize()
        expected_dequantized = torch.tensor([ 0.5039, -0.7559,  1.2441, -1.4961,  2.0000])
        assert dequantized.type() == torch.float32 or dequantized.type() == 'torch.FloatTensor', f"{quantization_format} should dequantize to float32."
        assert torch.allclose(dequantized, expected_dequantized, 0.001), f"Fail dequantizing vector from {quantization_format} format."

    elif quantization_format == "fp16":
        expected_quantized = vector.half()
        assert quantized.dtype == torch.float16, f"{quantization_format} quantization failed: Incorrect dtype."
        assert torch.equal(quantized, expected_quantized), f"Fail quantizing vector to {quantization_format} format."

        dequantized = qv.dequantize()
        expected_dequantized = expected_quantized.float()
        assert dequantized.dtype == torch.float32  or dequantized.type() == 'torch.FloatTensor', f"{quantization_format} should dequantize to float32."
        assert torch.allclose(dequantized, vector, atol=1e-3), f"{quantization_format} dequantization differs significantly from original vector."

    elif quantization_format == "1bit":
        expected_quantized = torch.tensor([ 1, -1,  1, -1,  1], dtype=torch.int8)
        assert quantized.dtype == torch.int8, f"{quantization_format} quantization failed: Incorrect dtype."
        assert torch.equal(quantized, expected_quantized), f"Fail quantizing vector to {quantization_format} format."
        # Expect an exception to be raised for 1bit and 1.58bit formats
        with pytest.raises(ValueError, match=f"Unsupported dequantization format: {quantization_format}"):
            qv = qv.dequantize()
    elif quantization_format == "1.58bit":
        expected_quantized = torch.tensor([ 0,  0,  1, -1,  1], dtype=torch.int8)
        assert quantized.dtype == torch.int8, f"{quantization_format} quantization failed: Incorrect dtype."
        assert torch.equal(quantized, expected_quantized), f"Fail quantizing vector to {quantization_format} format."
        # Expect an exception to be raised for 1bit and 1.58bit formats
        with pytest.raises(ValueError, match=f"Unsupported dequantization format: {quantization_format}"):
            qv = qv.dequantize()


@pytest.mark.parametrize("batch_size, input_size, weight_size", [(6, 3, 3), (6, 5, 5), (6, 3, 5), (3, 5, 5), (3, 3, 5)])
def test_mixed_precision_forward(batch_size, input_size, weight_size):
    """Tests the mixed_precision_forward function for accuracy of the output."""
    torch.manual_seed(42)  # Set a fixed seed for reproducibility
    # Generate random input and weight tensors
    input = torch.rand((batch_size, input_size), dtype=torch.float32)  # values between -1 and 1
    input[0, 0] = 6.0  # introduce a large value to test thresholding
    input[1, 2] = -6.0  # introduce a large negative value to test thresholding
    weight = torch.rand((input_size, weight_size), dtype=torch.float32) * 2   # values between -2 and 2

    # Perform mixed precision forward computation
    output = mixed_precision_forward(input, weight)

    # Check that output is produced and in FP32 format
    assert output is not None, "No output generated by mixed_precision_forward."
    assert output.dtype == torch.float32, "Output from mixed_precision_forward is not in FP32 format."

    # Verify accuracy of the mixed precision result by comparing with regular FP32 matrix multiplication
    expected_output = torch.matmul(input, weight)
    assert torch.allclose(output, expected_output, rtol=0.05), "Mixed precision result deviates significantly from FP32 multiplication."

if __name__ == "__main__":
    pytest.main()
